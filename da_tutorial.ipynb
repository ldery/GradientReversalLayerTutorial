{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Domain Adaptation By Backpropagation #\n",
    "We are given a source dataset [$(x_{i}, y_{i}) \\in D_{S} $] where $x_{i}$ is an instance and $y_{i}$ is the corresponding label. \n",
    "Our task is to classify the target dataset [$x_{i} \\in D_{T}$] for which we have no labels. Thus we wish to perform unsupervised classification on $D_{T}$ by leveraging $D_{S}$ via Domain Adaptation. To do this, we will leverage insights from the paper \"Unsupervised Domain Adaptation By Backpropagation\" https://arxiv.org/pdf/1409.7495.pdf\n",
    "\n",
    "The structure of this assignment is as follows.\n",
    "\n",
    "**Part 1 : Upper Bounding Performance**\n",
    "\n",
    "You will build a supervised classifier for $D_{S}$ and $D_{T}$, given the full datasets. This will provide an upper bound on the our performance on  $D_{T}$ in the case where we have training labels. \n",
    "\n",
    "**Part 2 : Lower Bounding Performance**\n",
    "\n",
    "You will apply the classifier from $D_{S}$ to $D_{T}$. Here, we are assuming that training a supervised classifier on $D_{S}$ (for which we have lables) and applying it directly to $D_{T}$ is sufficient. \n",
    "The performance of this method should provide a lower bound. \n",
    "\n",
    "**Part 3 : Gradient Reversal Layer**\n",
    "\n",
    "In this part, we apply the trick from [1] to peform unsupervised classification on $D_{T}$ by leveraging the available data in $D_{S}$. We will do this by training a model that has 2 heads. Head 1 will be used for performing classification - this will only be trained on $D_{S}$ since it has available labels. Head 2 will be used for distinguising the two domains $D_{S}$ and $D_{T}$. It turns out that by trying to produce representations that fool Head 2, you can build a reasonably accurate classifier for $D_{T}$  \n",
    "\n",
    "[1] \"Unsupervised Domain Adaptation By Backpropagation\" https://arxiv.org/pdf/1409.7495.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 : Upper Bounding Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import datasets \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import DataLoader \n",
    "from data_loader import MNISTM\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_transform = transforms.ToPILImage()\n",
    "to_tensor_tform = transforms.ToTensor() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load MNIST**\n",
    "\n",
    "The MNIST Dataset is a set of grayscale images of digits from 0-9. \n",
    "\n",
    "Run the Cell below to load the training and test splits of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root='./mnist', train=True, transform=to_tensor_tform, download=True)\n",
    "mnist_test = datasets.MNIST(root='./mnist', train=False, transform=to_tensor_tform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_img, mnist_label = mnist_train[0]\n",
    "print('Mnist Images have this shape : ', mnist_img.shape)\n",
    "plt.imshow(pil_transform(mnist_img))\n",
    "_ = plt.title('Digit : {}'.format(mnist_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load MNIST-M**\n",
    "\n",
    "MNISTM is a dataset generated by placing digits from 0-9 on random background patches.\n",
    "\n",
    "Run the Cell below to load the training and test splits of MNIST-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistm_train = MNISTM(root='./mnistm', mnist_root='./mnist', train=True, transform=to_tensor_tform, download=True)\n",
    "mnistm_test = MNISTM(root='./mnistm', mnist_root='./mnist', train=False, transform=to_tensor_tform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistm_img, mnistm_label = mnistm_train[0]\n",
    "print('Mnist Images have this shape : ', mnistm_img.shape)\n",
    "plt.imshow(pil_transform(mnistm_img))\n",
    "_ = plt.title('Digit : {}'.format(mnistm_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Specific Classifier ##\n",
    "Train a classifier for each of the individual domains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring constants here \n",
    "NCLASSES = 10 # Because MNIST, MNIST-M have 10 classes (0 - 9)\n",
    "NEPOCHS = 8   # Number of epochs to run the model for \n",
    "BATCH_SZ = 32 # The batch size of for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO [YOU]**\n",
    "\n",
    "Build a classifier for the MNIST and MNIST-M datasets. Feel free to design your own architecture. However, if you want, you can use the following architecture : \n",
    "\n",
    "Conv (output_channels = 8, kernel = 3)  \n",
    "|  \n",
    "ReLU  \n",
    "|  \n",
    "Conv (output_channels = 16, kernel = 5)  \n",
    "|  \n",
    "ReLU  \n",
    "|  \n",
    "Maxpool (kernel = 2, stride = 2)  \n",
    "|  \n",
    "Conv (output_channels = 32, kernel = 3)  \n",
    "|  \n",
    "ReLU  \n",
    "|  \n",
    "Conv (output_channels = 32, kernel = 5)  \n",
    "|  \n",
    "Maxpool (kernel = 2, stride = 2)  \n",
    "|  \n",
    "Flatten  \n",
    "|  \n",
    "Linear (in = 128 , out = NCLASSES)\n",
    "\n",
    "If not specified, all other parameters to these functions are the default. \n",
    "\n",
    "inchannels specifies the number of channels the input image has. Remember MNIST is grayscale (inchannels = 1) and MNIST-M is color (inchannels = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Flatten \n",
    "\n",
    "def get_domain_specific_model(inchannels=3):\n",
    "    ######################## YOUR CODE BEGINS ########################\n",
    "    domain_model = None\n",
    "    ######################## YOUR CODE ENDS ########################\n",
    "    assert domain_model != None, 'Domain Model has not yet been implemented'\n",
    "    domain_model.cuda()\n",
    "    return domain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(desc, train_stats, valid_stats):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.plot(train_stats[1], label='Train Acc', c='green')\n",
    "    ax1.plot(valid_stats[1], label='Valid Acc', c='red')\n",
    "    ax1.set_title('{} : Acccuracy Metric'.format(desc))\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(train_stats[0], label='Train Loss', c='green')\n",
    "    ax2.plot(valid_stats[0], label='Valid Loss', c='red')\n",
    "    ax2.set_title('{} : Loss Metric'.format(desc))\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def domain_epoch(epoch, domain_model, domain_data, optim, criterion, is_train=True): \n",
    "    print('Starting {} Epoch {}'.format(epoch, 'Train' if is_train else 'Valid'))\n",
    "    if is_train:\n",
    "        domain_model.train()\n",
    "    else:\n",
    "        domain_model.eval()\n",
    "    \n",
    "    losses, total_correct, total = [], 0, 0\n",
    "    for batch in tqdm_notebook(domain_data):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = torch.tensor(imgs).cuda(), torch.tensor(labels).cuda()\n",
    "        \n",
    "        logits = domain_model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        if is_train:\n",
    "            domain_model.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        total_correct += logits.argmax(dim=-1).eq(labels).sum().item()\n",
    "        total += logits.shape[0]\n",
    "    \n",
    "    avg_loss, avg_acc = np.mean(losses), (total_correct * 1.0) / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def model_pipeline(train_data, valid_data, inchannels=3, lr=3e-4):\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    domain_model = get_domain_specific_model(inchannels=inchannels)\n",
    "    parameters = list(domain_model.parameters())\n",
    "    optim = Adam(parameters, lr=lr)\n",
    "    train_stats, valid_stats = [[], []], [[], []]\n",
    "    for i in range(NEPOCHS):\n",
    "        t_loss_avg, t_acc_avg = domain_epoch(i, domain_model, train_data, optim, criterion, is_train=True)\n",
    "        train_stats[0].append(t_loss_avg)\n",
    "        train_stats[1].append(t_acc_avg)\n",
    "        v_loss_avg, v_acc_avg = domain_epoch(i, domain_model, valid_data, None, criterion, is_train=False)\n",
    "        valid_stats[0].append(v_loss_avg)\n",
    "        valid_stats[1].append(v_acc_avg)\n",
    "        \n",
    "    return domain_model, train_stats, valid_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Domain Specific Training**  \n",
    "Run the code below. The outputs of the model pipeline will be  \n",
    "1. A trained model\n",
    "2. An array containing the training statistics\n",
    "3. An array containing the validation statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Doing the domain specific training for MNIST')\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=BATCH_SZ)\n",
    "mnist_val_loader = DataLoader(mnist_test, batch_size=BATCH_SZ)\n",
    "mnist_model, mnist_train_stats, mnist_test_stats = model_pipeline(mnist_train_loader, mnist_val_loader, inchannels=1)\n",
    "visualize_results('MNIST', mnist_train_stats, mnist_test_stats)\n",
    "print(\"MNIST - Best Test Loss {}, Best Test Accuracy {}\".format(min(mnist_test_stats[0]), max(mnist_test_stats[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your test loss is decreasing and your test accuracy is increasing from the graphs above. \n",
    "Your Test Accuracy should be > 90 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Doing the domain specific training for MNIST-M')\n",
    "mnistm_train_loader = DataLoader(mnistm_train, batch_size=32)\n",
    "mnistm_val_loader = DataLoader(mnistm_test, batch_size=32)\n",
    "mnistm_model, mnistm_train_stats, mnistm_test_stats = model_pipeline(mnistm_train_loader, mnistm_val_loader, inchannels=3)\n",
    "visualize_results('MNIST-M', mnistm_train_stats, mnistm_test_stats)\n",
    "print(\"MNISTM - Best Test Loss {}, Best Test Accuracy {}\".format(min(mnistm_test_stats[0]), max(mnistm_test_stats[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your test loss is decreasing and your test accuracy is increasing from the graphs above. \n",
    "Your Test Accuracy should be > 90 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Lower Bounding Performance ##\n",
    "\n",
    "In this section, you will perform Cross Domain Evaluation. This will set a lower bound to your classifier performance. \n",
    "\n",
    "**TODO [YOU]**  \n",
    "In part one, you trained 2 models, **mnist_model** and **mnistm_model**. You will evaluate **mnist_model** on the Mnist-M dataset. \n",
    "Remember that the Mnist-M data is color whilst the **mnist_model** was trained on grayscale images. You will have to apply the following transforms to Mnist-M images  \n",
    "1. GrayScale\n",
    "2. Convert from Image to Tensor\n",
    "\n",
    "After this, run 1 epoch of the mnist_model on the cross-domain Mnist-M dataset. Use the **domain_epoch** function defined above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## YOUR CODE BEGINS ########################\n",
    "mnistm_transforms = None\n",
    "######################## YOUR CODE ENDS ########################\n",
    "\n",
    "cross_mnistm_test = MNISTM(root='./mnistm', mnist_root='./mnist', train=False, transform=mnistm_transforms, download=True)\n",
    "cross_mnistm_val_loader = DataLoader(cross_mnistm_test, batch_size=32)\n",
    "\n",
    "cross_mnistm_train = MNISTM(root='./mnistm', mnist_root='./mnist', train=True, transform=mnistm_transforms, download=True)\n",
    "cross_mnistm_train_loader = DataLoader(cross_mnistm_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## YOUR CODE BEGINS ########################\n",
    "# Run an epoch of mnist_model on cross_mnistm_val_loader. Remember we only care about evaluation\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "loss_avg, acc_avg = None\n",
    "######################## YOUR CODE ENDS ########################\n",
    "print('Doing Cross Domain Evaluation Yields : Loss - {}, Accuracy - {}'.format(loss_avg, acc_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the cross-domain accuracy is very poor. ~ 50%. In the next section, we will explore an approach to improve this accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Gradient Reversal Layer ##\n",
    "\n",
    "In this section, we will implement an unsupervised approach for domain adaptation - \"Unsupervised Domain Adaptation By Backpropagation\" https://arxiv.org/pdf/1409.7495.pdf.  \n",
    "![Unsupervised Domain Adaptation By Backpropagation](figs/paper_fig.png)\n",
    "\n",
    "The main idea behind this paper is to train 1 model to classify both $D_{S}$ and $D_{T}$.  \n",
    "Our model will have 1 head for classifying samples from both domains. This head, $G_y$ with parameters $\\theta_y$, will classify samples into NCLASSES where NCLASSES = 10 for our running example of the digit classification task. \n",
    "In order for $G_y$ to be a good classifier for both domains, we need to project $D_{S}$ and $D_{T}$ into a single, domain-invariant space **F** (the hope is that this space captures relevant information for classifying digits and discards irrelevant information like background). What this means is that after embedding $D_{S}$ and $D_{T}$ into **F**, we should not be able to distinguish samples from the two domains.\n",
    "Our model has a trunk $G_{f}$ with parameters $\\theta_f$ that is responsible for embedding samples into the domain invariant space **F**. In order to ensure that the space defined by $G_{f}$ is domain-invariant, we augment our model with another head $G_d$ with parameters $\\theta_d$ which is responsible identifying the domain of samples from **F**. Thus, whilst $G_d$ tries to achieve maximum domain classification accuracy, $G_f$ tries to achieve minimal accuracy so that **F** is truly domain invariant.  \n",
    "The above setup thus defines a kind of min-max adversarial game between $G_f$ and $G_d$.\n",
    "\n",
    "More formally, let $L_d$ be the domain classification loss and $L_y$ be the digit classification loss. Let $d_i = 0$ if instance $x_i$ belongs to the source domain and $d_i = 1$ if it belongs to the target domain. Our overall loss function will be :  \n",
    "\n",
    "![Loss](figs/loss_fig.png)\n",
    "\n",
    "Running gradient descent on this loss will not work. Remember that we want a min-max adversarial game between $G_f$ and $G_d$ on the domain loss $L_d$ but with the current objective, both minimize $L_d$. Since we want $G_f$ to maximize $L_d$ instead, we have to perform gradient ascent instead of gradient descent on $G_f$ w.r.t $L_d$. \n",
    "This is achieved by inserting a Gradient Reversal Layer (GRL) ($R_\\lambda$ in the figure above) between $G_f$ and $G_d$. $R_\\lambda$ reverses the gradient from the domain classification head before they reach the feature trunk - as in figure 1 above. We introduce the factor $\\lambda$ which serves as a weighting between the classification loss and the domain loss for $G_f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO [YOU]**  \n",
    "Implement a Gradient Reversal Layer. You can extend PyTorch with a new Autograd Layer by following the examples here :\n",
    "https://pytorch.org/docs/stable/notes/extending.html\n",
    "\n",
    "Your **forward** function should take in **input**, **lambda**. Your forward pass should not modify the input but just save the required information for the backward pass.  \n",
    "\n",
    "![GRL](figs/grl_fig.png)\n",
    "\n",
    "Your **backward** function should return the **gradient of the input, scaled by $-\\lambda$** and **a dummy gradient of the lambda** which you can set to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "class GradRevLayer(Function):\n",
    "    ######################## YOUR CODE BEGINS ########################\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, lambda_):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pass\n",
    "    ######################## YOUR CODE ENDS ##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO [YOU]**  \n",
    "Implement the Full Gradient Reversal Model. Follow the following architecture in your **init** method. \n",
    "![architecture](figs/arch.png)\n",
    "\n",
    "You can instantiate a GRL via :  \n",
    "\n",
    "grad_rev_layer = GradRevLayer.apply\n",
    "\n",
    "Your **forward** method should take the input example **x** and the current value of  **lambda_** and return 3 things in the following order  \n",
    "1. the logits for the digit classification task\n",
    "2. the logits for the domain classification task \n",
    "3. the features from $G_f$ before they are passed through the gradient reversal layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReversalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GradReversalModel, self).__init__()\n",
    "        ######################## YOUR CODE BEGINS ########################\n",
    "        pass\n",
    "        ######################## YOUR CODE ENDS ##########################\n",
    "    \n",
    "    def forward(self, x, lambda_):\n",
    "        ######################## YOUR CODE BEGINS ########################\n",
    "        pass\n",
    "        ######################## YOUR CODE ENDS ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model, dataset, max_examples=1000):\n",
    "    feature_list, num_examples = [], 0\n",
    "    for batch in dataset:\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = torch.tensor(imgs).cuda(), torch.tensor(labels).cuda()\n",
    "        dummy_lambda_ = torch.tensor(1).cuda()\n",
    "        _, _, features = model(imgs, dummy_lambda_)\n",
    "        feature_list.append(features)\n",
    "        num_examples += imgs.shape[0]\n",
    "        if num_examples >  max_examples:\n",
    "            break\n",
    "    return torch.cat(feature_list)\n",
    "\n",
    "def tsne_visualize(final_source, final_target, init_source, init_target):\n",
    "    tsne =  TSNE(n_components=2)\n",
    "\n",
    "    init_embeds = tsne.fit_transform(torch.cat([init_source, init_target]).detach().cpu().numpy())\n",
    "    final_embeds = tsne.fit_transform(torch.cat([final_source, final_target]).detach().cpu().numpy())\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    src_len = init_source.shape[0]\n",
    "    ax1.scatter(init_embeds[:src_len, 0], init_embeds[:src_len, 1], marker='.', color='green', label='source')\n",
    "    ax1.scatter(init_embeds[src_len:, 0], init_embeds[src_len:, 1], marker='.', color='red', label='target')\n",
    "    ax1.set_title(\"Initial Distribution Before Domain Adaptation\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    src_len = final_source.shape[0]\n",
    "    ax2.scatter(final_embeds[:src_len, 0], final_embeds[:src_len, 1], marker='.', color='green', label='source')\n",
    "    ax2.scatter(final_embeds[src_len:, 0], final_embeds[src_len:, 1], marker='.', color='red', label='target')\n",
    "    ax2.set_title(\"Final Distribution After Domain Adaptation\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsup_da_epoch(epoch, model, source_data, target_data, optim, class_criterion, domain_criterion, is_train=True): \n",
    "    print('Starting {} Epoch {}'.format(epoch, 'Train' if is_train else 'Valid'))\n",
    "    \n",
    "    # Set the model to train or eval mode\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    def get_lambda(train_progress, gamma=10):\n",
    "        lambda_ =  ( 2 / (1 + math.exp(-gamma * train_progress) ) ) - 1\n",
    "        lambda_ = torch.tensor(lambda_, requires_grad=False).cuda()\n",
    "        return lambda_\n",
    "        \n",
    "    def run_batch(batch, lambda_, is_source=True):\n",
    "        imgs, labels = batch \n",
    "        imgs, labels = torch.tensor(imgs).cuda(), torch.tensor(labels).cuda()\n",
    "        \n",
    "        class_logits, domain_logits, _ = model(imgs, lambda_)\n",
    "        class_loss = class_criterion(class_logits, labels)\n",
    "        if is_source : \n",
    "            domain_labels = torch.ones_like(domain_logits)\n",
    "            domain_loss = domain_criterion(domain_logits, domain_labels)\n",
    "            total_loss = class_loss + domain_loss\n",
    "        else:\n",
    "            domain_labels = torch.zeros_like(domain_logits)\n",
    "            domain_loss = domain_criterion(domain_logits, domain_labels)\n",
    "            total_loss = domain_loss\n",
    "\n",
    "        if is_train:\n",
    "            total_loss.backward()\n",
    "\n",
    "        class_loss = class_loss.item()\n",
    "        class_accuracy = class_logits.argmax(dim=-1).eq(labels).sum().item()\n",
    "        \n",
    "        domain_loss = domain_loss.item()\n",
    "        domain_accuracy = ((F.sigmoid(domain_logits) > 0.5).long()).eq(domain_labels.long()).sum().item()\n",
    "        \n",
    "        return class_loss, class_accuracy, domain_loss, domain_accuracy, labels.shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    source_stats = [[], [], 0, 0, 0]\n",
    "    target_stats = [[], [], 0, 0, 0]\n",
    "\n",
    "    target_data = iter(target_data)\n",
    "    num_batches, cur_batch  = len(source_data), 1\n",
    "    for batch in tqdm_notebook(source_data):\n",
    "        lambda_ = get_lambda((num_batches * epoch + cur_batch ) / (num_batches * NEPOCHS))\n",
    "        class_loss, class_corr, domain_loss, domain_corr, sz = run_batch(batch, lambda_)\n",
    "        \n",
    "        # Save the statistics of the batch\n",
    "        source_stats[0].append(class_loss)\n",
    "        source_stats[2] += class_corr\n",
    "        source_stats[1].append(domain_loss)\n",
    "        source_stats[3] += domain_corr\n",
    "        source_stats[4] += sz\n",
    "        \n",
    "        class_loss, class_corr, domain_loss, domain_corr, sz = run_batch(target_data.next(), lambda_, is_source=False)\n",
    "       \n",
    "        # Save the statistics of the batch\n",
    "        target_stats[0].append(class_loss)\n",
    "        target_stats[2] += class_corr\n",
    "        target_stats[1].append(domain_loss)\n",
    "        target_stats[3] += domain_corr\n",
    "        target_stats[4] += sz\n",
    "        \n",
    "        if is_train:\n",
    "            optim.step()\n",
    "            model.zero_grad()\n",
    "        \n",
    "        cur_batch += 1\n",
    "        \n",
    "    source_stats = np.mean(source_stats[0]),  source_stats[2] * 1.0 /  source_stats[4], np.mean(source_stats[1]), source_stats[3] * 1.0 / source_stats[4]\n",
    "    target_stats = np.mean(target_stats[0]),  target_stats[2] * 1.0 /  target_stats[4], np.mean(target_stats[1]), target_stats[3] * 1.0 / target_stats[4]        \n",
    "    print('Source : digit loss {} | digit accuracy {} | domain loss {} | domain accuracy {}'.format(*source_stats))\n",
    "    print('Target : digit loss {} | digit accuracy {} | domain loss {} | domain accuracy {}'.format(*target_stats))\n",
    "    return source_stats, target_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO[YOU]**  \n",
    "What type of loss should be used for digit classification   \n",
    "What type of loss should be used for domain classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsup_da_model_pipeline(model, source_train, source_valid, target_train, target_valid, lr=5e-5):\n",
    "    ######################## YOUR CODE BEGINS ########################\n",
    "    class_criterion = None \n",
    "    domain_criterion = None\n",
    "    ######################## YOUR CODE ENDS ##########################\n",
    "    parameters = list(model.parameters())\n",
    "    optim = Adam(parameters, lr=lr)\n",
    "    source_train_stats, source_valid_stats = [], []\n",
    "    target_train_stats, target_valid_stats = [], []\n",
    "    for i in range(NEPOCHS):\n",
    "        # Perform Traininig Procedure\n",
    "        source_stats, target_stats = unsup_da_epoch(i, model, source_train, target_train, optim, class_criterion, domain_criterion, is_train=True)\n",
    "        source_train_stats.append(source_stats)\n",
    "        target_train_stats.append(target_stats)\n",
    "        \n",
    "        # Perform Evaluation Procedure\n",
    "        source_stats, target_stats = unsup_da_epoch(i, model, source_valid, target_valid, None, class_criterion, domain_criterion, is_train=False)\n",
    "        source_valid_stats.append(source_stats)\n",
    "        target_valid_stats.append(target_stats)\n",
    "        \n",
    "    return model, source_train_stats, source_valid_stats, target_train_stats, target_valid_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tform = transforms.Compose([ \n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Lambda(lambda x: torch.cat([x, x, x], 0)),\n",
    "                          ])\n",
    "\n",
    "mnist_train = datasets.MNIST(root='./mnist', train=True, transform=tform, download=True)\n",
    "mnist_test = datasets.MNIST(root='./mnist', train=False, transform=tform, download=True)\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=BATCH_SZ)\n",
    "mnist_val_loader = DataLoader(mnist_test, batch_size=BATCH_SZ)\n",
    "\n",
    "tform = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                          ])\n",
    "\n",
    "cross_mnistm_test = MNISTM(root='./mnistm', mnist_root='./mnist', train=False, transform=tform, download=True)\n",
    "cross_mnistm_train = MNISTM(root='./mnistm', mnist_root='./mnist', train=True, transform=tform, download=True)\n",
    "cross_mnistm_train_loader = DataLoader(cross_mnistm_train, batch_size=BATCH_SZ)\n",
    "cross_mnistm_val_loader = DataLoader(cross_mnistm_test, batch_size=BATCH_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  GradReversalModel().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the initial features from the model before training \n",
    "init_source = get_features(model, mnist_val_loader)\n",
    "init_target = get_features(model, cross_mnistm_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEPOCHS = 20\n",
    "BATCH_SZ = 64\n",
    "# TRAIN THE GRL MODEL. \n",
    "model, source_train_stats, source_valid_stats, target_train_stats, target_valid_stats = unsup_da_model_pipeline(model, mnist_train_loader, mnist_val_loader, cross_mnistm_train_loader, cross_mnistm_val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the target digit accuracy as the model trains. How does the best target model accuracy compare with the cross-domain evaluation done above ? \n",
    "You should be getting an accuracy ~70% with your GRL Model. This is about ~20% more than the cross-domain evaluation accuracy.   \n",
    "Run the Cell below to visualize the accuracies and losses at durring training for both the source and target. Do the curves follow your expectations ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_c_loss, train_c_acc, train_d_loss, train_d_acc = list(zip(*source_train_stats))\n",
    "valid_c_loss, valid_c_acc, valid_d_loss, valid_d_acc = list(zip(*source_valid_stats))\n",
    "\n",
    "visualize_results('MNIST CLASS', [train_c_loss, train_c_acc,], [valid_c_loss, valid_c_acc])\n",
    "visualize_results('MNIST DOMAIN', [train_d_loss, train_d_acc,], [valid_d_loss, valid_d_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_c_loss, train_c_acc, train_d_loss, train_d_acc = list(zip(*target_train_stats))\n",
    "valid_c_loss, valid_c_acc, valid_d_loss, valid_d_acc = list(zip(*target_valid_stats))\n",
    "\n",
    "visualize_results('MNISTM CLASS', [train_c_loss, train_c_acc,], [valid_c_loss, valid_c_acc])\n",
    "visualize_results('MNISTM DOMAIN', [train_d_loss, train_d_acc,], [valid_d_loss, valid_d_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the instructions above, we want $G_f$ to embed samples into a domain-invariant space **F**. \n",
    "Run the above cell to visualize the features from $G_f$ before and after training the GRL model. \n",
    "What do you notice. Is this what you expect ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_source = get_features(model, mnist_val_loader)\n",
    "final_target = get_features(model, cross_mnistm_val_loader)\n",
    "tsne_visualize(final_source, final_target, init_source, init_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
